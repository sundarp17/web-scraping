Abstract
"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  "
Scaling conditional random fields for natural language processing Terms and Conditions: Terms and Conditions: Copyright in works deposited in Minerva Access is retained by the
"The paper addresses the issue of cooperation between linguistics and natural language processing (NLP), in general, and between linguistics and machine translation (MT), in particular. It focuses on just one direction of such cooperation, namely applications of linguistics to NLP, virtually ignoring"
"In most natural language processing applications, Description Logics have been used to encode in a knowledge base some syntactic, semantic, and pragmatic elements needed to drive the semantic interpretation and the natural language generation processes. More recently, Description Logics have been used to fully characterise the semantic issues involved in the interpretation phase. In this Chapter the various proposals appeared in the literature about the use of Description Logics for natural language processing will be analysed.  15.1 "
"Introduction  Statistical natural language processing (SNLP)    is a field lying in the intersection of natural language processing and machine learning. SNLP di#ers from traditional natural language processing in that instead of having a linguist manually construct some model of a given linguistic phenomenon, that model is instead (semi-) automatically constructed from linguistically annotated resources. Methods for assigning partof -speech tags to words, categories to texts, parse trees to sentences, and so on, are (semi-) automatically acquired using machine learning techniques.  The recent trend of applying statistical techniques to natural language processing came largely from industrial speech recognition research groups at AT&T's Bell Laboratories and IBM's T.J. Watson Research Center. Statistical techniques in speech recognition have so vastly outstripped the performance of their non-statistical counterparts that rule-based speech recognition systems are essentially no longer a"
"Robots that interact with humans face-to-face using natural language need to be responsive to the way humans use language in those situations. We propose a psychologicallyinspired natural language processing system for robots which performs incremental semantic interpretation of spoken utterances, integrating tightly with the robotâ€™s perceptual and motor systems."
"Natural languages are languages spoken by humans. Currently we are not yet at the point where these languages in all of their unprocessed forms can be understood by computers. Natural language processing is the collection of techniques employed to try and accomplish that goal. The field of natural language processing (NLP) is deep and diverse, This paper will introduce natural language understanding and generation to the reader then go in depth on how these topics work and relate to NLP as a whole. Furthermore, this paper will discuss the applications and challenges of NLP, namely duplicate error report detection, tutoring systems, and database interfaces. 1."
"ABSTRACT: Language is way of communicating your words Language helps in understanding the world,we get a better insight of the world. Language helps speakers to be as vague or as precise as they like. NLP Stands for natural language processing.. Natural languages are those languages that are spoken by the people.Natural language processing girdles everything a computer needs to understand natural language and also generates natural language.Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics mainly focuses on the interactions between computers and human languages or natural languages. NLP is focussed on the area of human computer interaction. The need for natural language processing was also felt because there is a wide storage of information recorded or stored in natural language that could be accessible via computers. Information is constantly generated in the form of books, news, business and government reports, and scientific papers, many of which are available online or even in some reports. A system requiring a great deal of information must be able to process natural language to retrieve much of the information available on computers. Natural language processing is an interesting and difficult field in which we have to develop and evaluate or analyse representation and reasoning theories. All of the problems of AI arise in this domain; solving ""the natural language problem "" is as difficult as solving ""the AI problem "" because any field can be expressed or can be depicted in natural language."
"Natural Language Processing The subject of Natural Language Processing can be considered in both broad and narrow senses. In the broad sense, it covers processing issues at all levels of natural language understanding, including speech recognition, syntactic and semantic analysis of sentences, reference to the discourse context (including anaphora, inference of referents, and more extended relations of discourse coherence and narrative structure), conversational inference and implicature, and discourse planning and generation. In the narrower sense, it covers the syntactic and semantic processing sentences to deliver semantic objects suitable for referring, inferring, and the like. Of course, the results of inference and reference may under some circumstances play a part in processing in the narrow sense. But the processes that are characteristic of these other modules are not the primary"
"We report experiments on the use of standard natural language processing (NLP) tools for the analysis of music lyrics. A significant amount of music audio has lyrics. Lyrics encode an important part of the semantics of a song, therefore their analysis complements that of acoustic and cultural metadata and is fundamental for the development of complete music information retrieval systems. Moreover, a textual analysis of a song can generate ground truth data that can be used to validate results from purely acoustic methods. Preliminary results on language identification, structure extraction, categorization and similarity searches suggests that a lot of profit can be gained from the analysis of lyrics."
"this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging"
"The paper summarizes the essential properties of document retrieval and reviews both conventional practice and research findings, the latter suggesting that simple statistical techniques can be effective. It then considers the new opportunities and challenges presented by the ability to search full text directly (rather than e.g. titles and abstracts), and suggests appropriate approaches to doing this, with a focus on the role of natural language processing. The paper also comments on possible connections with data and knowledge retrieval, and concludes by emphasizing the importance of rigorous performance testing. This paper will appear in Communications of the ACM. 2 Introduction Automatic text, or document, retrieval has recently become a topic of interest for those working in natural language processing (NLP). The aim of this article is to indicate the key properties of document retrieval, distinguishing it from both data retrieval and question answering; to summarize past exper..."
"This paper focuses on connectionist models in natural language processing. We briefly present and discuss several aspects of high level tasks which recently have been approached with connectionism, either with localist or parallel distributed processing models. Several interesting architectures have been proposed in the last decade, and connectionist natural language processing seems like a promising area for future research in artificial intelligence. 1. Introduction  The field of Natural Language Processing (NLP) is being approached in several different ways. Most NLP systems so far have been implemented with a symbolic approach, which is the more ""traditional"" approach to Artificial Intelligence (AI). After the revival of connectionism in the 1980s, when Rumelhart & McClelland[20] demonstrated that it was possible to get around the limitations which Minsky & Papert[15] pointed out in the 1960s, papers on connectionist approaches to NLP started to appear. Currently, it is well known ..."
Abstract: The article explores the possibility to construct a unified word feature out of the component features of letters. Each letter is modeled by a different attractor and finally embedded in a quadratic iterated map. The result is the word feature that can account for the meaning extraction process of language understanding. This is a new approach in natural language processing based on the deterministic chaotic behavior of dynamical systems. 1
"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
"this paper (see [Schank 86] for a theoretical discussion and [Kass 86] and [Leake and Owens 86] for brief discussions of a program built around these .principles); the goal here is simply  to point out how our interest in natural language processing has led us naturally, and indeed inevitably, to develop theories of explanation and creativity. Some may say that we have strayed from the core issues of NLP, but our point is that these are the core issues. The drive to eplain what might be 'going on in the stow guides the entire understanding process"
"Objectives To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. Target audience This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art. Scope We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundationâ€™s Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field."
"This paper briefly describes the current implementation status of an intelligent information retrieval system, MARIE, that employs natural language processing techniques. Descriptive captions are used to iden- tify photographic images concerning various military projects. The captions are parsed to produce a logical form from which nouns and verbs are extracted to form the primary keywords. User queries are also specified in natural language. A two-phase search process employing coarse-grain and fine-grain match processes is used to find the captions that best match the query. A type hierarchy based on object-oriented programming constructs is used to represent the semantic knowledge base. This knowledge base contains knowledge of various military concepts and terminology with specifics from the Naval Weapons Center. Methods are used for creating the logical form during semantic analysis, generating the keywords to be used in the coarse-grain match process, and fine-grain matching between query and caption logical forms."
"Abstract: Metabolism is the machinery of life and signal transduction provides the regulatory mechanisms to control that machinery. Due to the complexity of signal transduction pathways, computational approaches are needed to aid the biologist in integrating available knowledge and in the formulation of testable hypotheses. Our objective is to apply multi-agent systems to build a comprehensive system for study of signal transduction. The agent driven system draws inferences and hypothesizes pathways, evaluates predictions and identifies information relevant to signal transduction from both web based and literature resources. We describe here a system for agent directed natural language processing to extract information from journal articles. An interface was developed to permit curation of the NLP results and deposition of accepted results into a knowledge base. Motivation: The advent of high-throughput methods has revolutionized the field of biology, creating an information explosion in their wake. The daily accretion of experimental data from sequencing projects, scientific literature, gene array experiments and other high volume data pipelines, has prompted calls (Karp, 2001) to encode"
"Web has emerged as the most important source of information in the world. This has resulted in need for automated software components to analyze web pages and harvest useful information from them. However, in typical web pages the informative content is surrounded by a very high degree of noise in the form of advertisements, navigation bars, links to other content, etc. Often the noisy content is interspersed with the main content leaving no clean boundaries between them. This noisy content makes the problem of information harvesting from web pages much harder. Therefore, it is essential to be able to identify main content of a web page and automatically isolate it from noisy content for any further analysis. Most existing approaches rely on prior knowledge of website specific templates and hand-crafted rules specific to websites for extraction of relevant content. We propose a generic approach that does not require prior knowledge of website templates. While HTML DOM analysis and visual layout analysis approaches have sometimes been used, we believe that for higher accuracy in content extraction, the analyzing software needs to mimic a human user and under- stand content in natural language similar to the way humans intuitively do in order to eliminate noisy content. In this paper, we describe a combination of HTML DOM analysis and Natural Language Processing (NLP) techniques for automated extractions of main article with associated images from web pages."
"This report presents a detailed analysis and review of NLP evaluation, in principle and in practice. Part 1 examines evaluation concepts and establishes a framework for NLP system evaluation. This makes use of experience in the related area of information retrieval and the analysis also refers to evaluation in speech processing. Part 2 surveys significant evaluation work done so far, for instance in machine translation, and discusses the particular problems of generic system evaluation. The conclusion is that evaluation strategies and techniques for NLP need much more development, in particular to take proper account of the influence of system tasks and settings. Part 3 develops a general approach to NLP evaluation, aimed at methodologically-sound strategies for test and evaluation motivated by comprehensive performance factor identification. The analysis throughout the report is supported by extensive illustrative examples.  This work was carried out under the UK Science and Engineeri..."
"Abstract-- Natural Language Processing is a theoretically motivated range of computational techniques for analysing and representing naturally occurring texts at one or more levels of linguistic analysis for the purpose of achieving human-like language processing for a range of tasks or applications [1]. To perform natural language processing a variety of tools and platform have been developed, in our case we will discuss about NLTK for Python.The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language[2]. It provides easy-to-use interfaces to many corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. In this paper we discuss different approaches for natural language processing using NLTK."
"Abstractâ€”Natural language processing (NLP) is the application of automated parsing and machine learning techniques to analyze standard text. Applications of NLP to requirements engineering include extraction of ontologies from a requirements specification, and use of NLP to verify the consistency and/or completion of a requirements specification. This work-in-progress paper describes a new approach to the interpretation, organization and management of textual requirements through the use of application-specific ontologies and natural language processing. We also design and exercise a prototype software tool that implements the new framework on a simplified model of an aircraft."
"This paper reviews the processes involved in Natural Language Processing (NLP). It then demonstrates the various kinds of choices that need be taken during the execution of the word morphology, the syntactic text analysis, or text generation components. It compares the time complexity of traditional serial algorithms and examines the possible expected gain in some corresponding parallel counterparts. 1."
"This article focusses on the derivation of large lexicons for natural language processing. We describe the  development of a dictionary support environment linking a restructured version of the Longman  Dictionary of Contemporary English to natural language processing systems. The process of restructuring  the information in the machine readable version of the dictionary is discussed. The Longman  grammar code system is used to construct 'theory neutral' lexical entries. We demonstrate how such  lexical entries can be put to practical use by linking up the system described here with the experimental  PATR-II grammar development environment. Finally, we offer an evaluation of the utility of the  grammar coding system for use by automatic natural language parsing systems"
"We introduce a method for analyzing the complexity of natural language processing tasks, and for predicting the difficulty new NLP tasks. Our complexity measures are derived from the Kolmogorov complexity of a class of automata â€” meaning automata, whose purpose is to extract relevant pieces of information from sentences. Natural language semantics is defined only relative to the set of questions an automaton can answer. The paper shows examples of complexity estimates for various NLP programs and tasks, and some recipes for complexity management. It positions natural language processing as a subdomain of software engineering, and lays down its formal foundation. 1"
This is an author-produced version of a paper published in The
"Work in computational linguistics began very soon after the development of the first computers (Booth, Brandwood and Cleave 1958), yet in the intervening four decades there has been a pervasive feeling that progress in computer understanding of natural language has not been commensurate with progress in other computer applications. Recently, a number of prominent"
"In recent years, machine learning (ML) has been used more and more to solve complex tasks in different disciplines, ranging from Data Mining to Information"
"Deep learning has emerged as a new area of machine learning research. It tries to mimic the human brain, which is capable of processing and learning from the complex input data and solving different kinds of complicated tasks well. It has been successfully applied to several fields such as images, sounds, text and motion. The techniques developed from deep learning research have already been impacting the research of natural language process. This paper reviews the recent research on deep learning, its applications and recent development in natural language processing. 1"
". Information retrieval addresses the problem of finding those  documents whose content matches a user's request from among a large  collection of documents. Currently, the most successful general purpose  retrieval methods are statistical methods that treat text as little more  than a bag of words. However, attempts to improve retrieval performance  through more sophisticated linguistic processing have been largely unsuccessful.  Indeed, unless done carefully, such processing can degrade  retrieval effectiveness.  Several factors contribute to the difficulty of improving on a good statistical  baseline including: the forgiving nature but broad coverage of the  typical retrieval task; the lack of good weighting schemes for compound  index terms; and the implicit linguistic processing inherent in the statistical  methods. Natural language processing techniques may be more  important for related tasks such as question answering or document summarization.  1 Introduction  Imagine that you..."
""
Abstract-A system that recognizes and authenticates the voice of a user by extracting the distinct features of their voice samples is usually termed as Voice recognition system. Voice identification is carried out by converting the human voice into digital data. The digitized audio samples then undergo feature excerption process to extract Mel Frequency Cepstral Coefficients features. These coefficients are subjected to feature matching through Dynamic Time Warping to match with the patterns existing in the database for limited Tamil words. This paper focuses on a secure system that deploys the voice recognition for a natural language (Tamil) by combining the digital and mathematical knowledge using MFCC and DTW to extract and match the features to improve the accuracy for better performance.
Abstract: Testing against natural language requirements is the standard approach for system and acceptance testing. This test is often performed by an independent test organization unfamiliar with the application area. The only things the testers have to go by are the written requirements. So it is essential to be able to analyze those requirements and to extract test cases from them. In this paper an automated approach to requirements based testing is presented and illustrated on an industrial application.
Proceedings of the Workshop on
uni-hamburg.de
" This paper proposes a recognition method for Chinese Noun Phrase based on word co-occurrence directed graph. An input document is firstly scanned in which noun word string is retrieved. Atomic word table and word co-occurrence directed graph is then generated according to the word strings. A search is performed on the graph to find the longest paths with priority weight satisfying certain criteria. The word strings corresponding to the paths are considered as noun phrases. As dimensionality reduction is applied, the scale of the word co-occurrence directed graph is reduced significantly, and thus the efficiency of the algorithm is improved. Experimental results demonstrate that the precision of noun phrase recognition reaches 95.4%."
2 6
"In this Chapter the basic uses of Description Logics for Natural Language Processing will be analysed, together with a little bit of history, and the role of Description Logics in the current state of the art in computational linguistics will be pointed out.  18.1 Introduction  Since the early days of the Kl-One system, one of the main applications of Description Logics has been for semantic interpretation in Natural Language Processing [ Brachman et al., 1979 ] . Semantic interpretation is the derivation process from the syntactic analysis of an utterance to its logical form { intended as the representation of its literal deep and context-dependent meaning. Typically, Description Logics have been used to encode in a knowledge base both syntactic and semantic elements needed to drive the semantic interpretation process. A part of the knowledge base constitutes the lexical semantics knowledge, relating words and their syntactic properties to concept structures, while the other part desc..."
"algorithms that allow understanding and generation of humor. There is the general aim of modeling humor, and if we can do that, it will provide us with lots of information about our cognitive abilities in general, such as reasoning, remembering, understanding situations, and understanding conversational partners. But it also provides us with information about being creative, making associations, storytelling and language use. Many more subtleties in face-to-face and multiparty interaction can be added, such as using humor to persuade and dominate, to soften or avoid a face threatening act, to ease a tense situation or to establish a friendly or romantic relationship. One issue to consider is: when is a humorous act appropriate? This 2012 workshop is different from previous workshops [1,2]. The 1st and 2nd workshop on computational humor aimed at providing an opportunity to present scientific results on modelling humor, where modelling needs to be done in order to be able to understand humor and to generate humor in a context of human-computer interaction. The first workshop [1], organized at the University of Twente in September 1996, was an opportunity to listen to researchers and publicists such as Marvin Minsky, Douglas Hofstadter, and John Allen Paulos."
"Abstract Many information retrieval(IR) systems retrieve relevant documents based on exact matching of keywords between a query and documents. This method degrades precision rate. In order to solve the problem, we collected semantically related words and assigned semantic relationships used in general thesaurus and a special relationship called keyfact term(FT) manually. In addition to the semantic knowledge, we automatically constructed statistic knowledge based on the concept of mutual information. Keyfact is an extended concept of keyword represented by noun and compound noun. Keyfact can be a verb and an adjective including subject or object term. We first retrieved relevant documents with original query using tf * idf weighting formula and then an expanded query including keyfacts is used in both second document ranking and word sense disambiguating. So we made an improvement in precision rate using keyfact network. 1"
"This chapter considers the revolution that has taken place in natural language processing research over the last five years. It begins by providing a brief guide to the structure of the field and then presents a caricature of two competing paradigms of 1980s NLP research and indicates the reasons why many of those involved have now seen fit to abandon them in their pure forms. Attention is then directed to the lexicon, a component of NLP systems which started out as Cinderella but which has finally arrived at the ball. This brings us to an account of what has been going on in the field most recently, namely a merging of the two 1980s paradigms in a way that is generating a host of interesting new research questions. The chapter concludes by trying to identify some of the key conceptual, empirical and formal issues that now stand in need of resolution. 1.1 Introduction The academic discipline that studies computer processing of natural languages is known as natural language processing ..."
"Introduction  Patterns in music have been the object of intensive studies in the past years. \One of the purposes of analyzing musical structure and form is to discover the patterns that are explicit or implicit in musical works"" Simon [13]. Patterns comprise periodicity, make use of alphabets, can be compound (made up of subpatterns) and possess phrase structure with various forms of punctuation. Traditionally, composers have employed pattern propagation intuitively, but algorithmic composition techniques allow the pattern propagation to be formalized, albeit a high level. During composition, all the musical patterns evolve according to the rules and constraints specied at the design stage. In jazz improvisation, the musician invents a solo guided by a progression of chords (the changes). One approach [1] to learn improvising is to memorize patterns (short chunks of music) that t sub-progressions, and to concatenate them to form a whole solo that ts a whole progression. One"
"We argue that manual and automatic thesauruses are alternative resources for the same NLP tasks. This involves the radical step of interpreting manual thesauruses as classifications of words rather than word senses: the case for this is made. The range of roles for thesauruses within NLP is briefly presented and the WASPS thesaurus is introduced. Thesaurus evaluation is now becoming urgent. A range of evaluation strategies, all embedded within NLP tasks, is proposed."
"This paper surveys in short the activity of the Knowledge Representation and Reasoning group at IRST for Natural Language Processing. We have developed two Description Logic based systems to be used in large Natural Language dialogue architectures. The functional interaction of such KR systems with the other modules is briefly described. Then, several qualifying extensions of the basic systems are introduced, and their usefulness for natural language applications is explained."
this paper we argue that questionanswering  (QA) over technical domains  is distinctly different from TREC-based  QA or Web-based QA and it cannot  benefit lom data-intensive approaches
"SRI has developed a new architecture for integrating speech and natural-language processing that applies linguistic constraints during recognition by incrementally expanding the state-transition network embodied in a unification grammar. We compare this dynamic-gralnlnar-network (DGN) approach to its principal alternative, word-lattice parsing, presenting preliminary experimental results that suggest the DGN approach requires much less computation time than word-lattice parsing, while maintaining a very tractable recognition search space."
Universit&quot;at des Saarlandes
"Vast quantities of text are becoming available in elec-tronic form, ranging from published documents (e.g., electronic dictionaries, encyclopedias, libraries and archives for information retrieval services), to private databases (e.g., marketing information, legal records, medical histories), to personal email and faxes. Online information services are reaching mainstream computer users. There were over 15 million Internet users in 1993, and projections are for 30 million in 1997. With media attention reaching all-time highs, hardly a day goes by without a new article on the National Infor-mation Infrastructure, digital libraries, networked services, digital convergence or intelligent agents. This attention is moving natural language processing along the critical path for all kinds of novel applications. This article will mention a number of successful applications of natural language processing (NLP). Word processing and information management are two of the better examples, though there have been many others, both large and small. A small success, worth a few million or perhaps even a few tens of millions of dollars a year, is more than enough to support a small busi-ness. There have also been a few big successes, for example, $100 million or more a year in revenues, large enough to help create whole new industries. Of course, along with the successes, there have also been a few failures. We donâ€™t want to contribute to the â€œhypeâ€™ â€™ by mentioning only the success-es. The early boom and bust in machine translation (MT), starting with the first public demonstration of MT in 1954 and ending with the sobering find-ings of the ALPAC committee in 1966 [2], will be mentioned as an example of the danger of excessive optimism. While keeping these lessons of the past in mind, this article will describe a 71 A variety of suc-cessful applica-tions of natural language process-ing (NLP), both large and small, are surveyed in this article and new opportunities are explored."
"Over the last few years, a number of areas of natural language processing have begun applying graph-based techniques. These include, among others, text summarization, syntactic parsing, word sense disambiguation, ontology construction, sentiment and subjectivity analysis, text clustering. In this paper, we present some of the most successful graph-based representations and algorithms used in language processing, and try to explain how and why they work. 1"
"visual development environment to support the visual assembly, execution and analysis of modular natural language processing systems. The visual model is an executable data flow program graph, automatically synthesised from data dependency declarations of language processing modules. The graph is then directly executable: modules are run interactively in the graph, and results are accessible via generic text visualisation tools linked to the modules. These tools lighten the â€˜cognitive load â€™ of viewing and comparing module results by relating data produced by modules back to the underlying text, by reducing the amount of search in examining results, and by displaying results in context. Overall, the GATE integrated visual development environment leads to rapid understanding of system behaviour and hence to rapid system refinement, therefore demonstrating the utility of visual programming and visualisation techniques for the development of natural language processing systems. ï¿½ 2001 Academic Press"
"We applied a structure learning model, Max-Margin Structure (MMS), to natural language processing (NLP) tasks, where the aim is to capture the latent relationships within the output language domain. We formulate this model as an extension of multiâ€“class Support Vector Machine (SVM) and present a perceptronâ€“based learning approach to solve the problem. Experiments are carried out on two related NLP tasks: partâ€“ofâ€“speech (POS) tagging and machine translation (MT), illustrating the effectiveness of the model. 1."
"In Natural Language Processing (NLP), research results  from software engineering and software technology  have often been neglected."
"Many Natural Language Processing (NLP) techniques have been used in Information Retrieval. The results are not encouraging. Simple methods (stopwording, porter-style stemming, etc.) usually yield significant improvements, while higher-level processing (chunking, parsing, word sense disambiguation, etc.) only yield very small improvements or even a decrease in accuracy. At the same time, higher-level methods increase the processing and storage cost dramatically. This makes them hard to use on large collections. We review NLP techniques and come to the conclusion that (a) NLP needs to be optimized for IR in order to be effective and (b) document retrieval is not an ideal application for NLP, at least given the current state-of-the-art in NLP. Other IR-related tasks, e.g., question answering and information extraction, seem to be better suited. 1"
"Natural language is a complex and compound organization that structures basic linguistic elements to represent various meanings. Therefore, to understand the nature of natural language, we need a sophisticated treatment of the basic elements as well as the insights about how these elements will be structured. In the words of statistical natural language processing, we need a sophisticated statistical model of the basic elements, such as words or phrases, to be combined with the structural modeling such as syntactic parsing or dependency analysis. Since the basic property of these elements is considered common over the whole corpora, we need to model their distributional property over the corpora by a statistical learning approach. In this dissertation, I focus on the statistical learning approach to the dis-tributional modeling of basic elements by considering two kinds of distributional units, that is, static units and dynamic units. Along these two units, I propose novel treatments of distributional units that are dierent from the methods used in natural language processing so far. In Chapter 2, I consider static units. Static units are the units we know a priori to be eective for a specic kind of task in hand; for example, documents, paragraphs, or sentences. For the treatment of these units, specically, measuring a distance between two units, kernel methods have been adopted recently. However, not all the"
"Kernelized sorting is an approach for matching objects from two sources (or domains) that does not require any prior notion of similarity between objects across the two sources. Unfortunately, this technique is highly sensitive to initialization and high dimensional data. We present variants of kernelized sorting to increase its robustness and performance on several Natural Language Processing (NLP) tasks: document matching from parallel and comparable corpora, machine transliteration and even image processing. Empirically we show that, on these tasks, a semi-supervised variant of kernelized sorting outperforms matching canonical correlation analysis."
"In this paper, we describe a framework for developing probabilistic classifiers in natural language processing. Our focus is on formulating models that capture the most important interdependencies among features, to avoid overfitting the data while also characterizing the data well. The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics. Our goal is to make this model selection framework accessible to researchers in NLP, and provide pointers to available software and important references. In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model, and the parameter estimates) can be separately evaluated. We also demonstrate the classification performance of these models in a large-scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus (Hanks 1996). In 10-fold cross-validations, the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them"
"The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR. 1"
"The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR. 1"
Abstract- This paper explains the information retrieval using natural language processing for Malayalam language in these basic
"While computational logic has become widely used for representing and reasoning with linguistic knowledge, the cross-fertilization between logic programming and machine learning has given rise to a new discipline known as inductive logic programming. Inspired by, and building on the achievements of logic programming within both natural language research and machine learning, we point out opportunities for induction of linguistic knowledge within logic (programming).  Keywords: inductive logic programming, natural language processing, logic programming, machine learning. 1 Introduction  There is a growing interest amongst both linguistic engineers and machine learning researchers for applying symbolic learning algorithms in natural language R&D  1  . Linguists confronted with the high cost of development of essential resources are drawn towards machine learning in search for generic technologies to exploit corpora for system training purposes. Vice versa machine learning researchers are..."
"Information retrieval is the process of finding the documents in a document collection that satisfies the information need of the user. The documents are natural language constructs, and the motivation of this work is to investigate how natural language processing can be used to improve the performance of information retrieval systems. An abstract was supplied to the natural language processor TUC together with the necessary knowledge needed for TUC to understand the abstract. Then TUC was asked to determine the relevance of the abstract to some queries, and it was investigated how much extra knowledge needed to be supplied to TUC, in order to get the correct answer.  i Preface  This work is done as a term paper in the PhD-course Natural Language Interfaces at the Norwegian Institute of Technology during the fall of 1994. I wish to thank Associate Professor Tore Amble for always being prepared to answer questions concerning TUC that came up during the project. Trondheim, January 25, 1..."
"What is a statistical method and how can it be used in natural language  processing (NLP)? In this paper, we start from a definition of NLP as concerned with  the design and implementation of effective natural language input and output components  for computational systems. We distinguish three kinds of methods that are  relevant to this enterprise: application methods, acquisition methods, and evaluation  methods. Using examples from the current literature, we show that all three kinds  of methods may be statistical in the sense that they involve the notion of probability  or other concepts from statistical theory. Furthermore, we show that these statistical  methods are often combined with traditional linguistic rules and representations. In  view of these facts, we argue that the apparent dichotomy between ""rule-based"" and  ""statistical"" methods is an over-simplification at best."
"Natural Language Processing (NLP) is a very large and diverse subtopic of artificial intelligence. As a result, NLP itself has many subtopics including optical character recognition, text to speech translators, foreign language reading and writing aids, machine translation, and speech recognition. While these branches of NLP have some common problems that must be overcome to achieve the ultimate goal of NLP, each of these subtopics has unique roadblocks as well. While some of the problems that exist within these sub-domains have been solved, exciting research is currently being done in every one of these areas. The aim of this survey is to provide a brief history of NLP, motivate the importance of NLP and to explain the problems that just about all subtopics of NLP share. This paper will then examine one of the sub-topics more closely, highlighting previous research that has produced significant findings, and provide insight into the status of current research being done. 1."
"Traditional approaches tointerpretation in natural language processing typically fall into one of three classes: syntax-driven, semantics-driven, or frame/task based. Syntax-driven approaches use a domain-independent grammar to drive the interpretation process and produce a global parse of the input, accounting for each word of the sentence. Semantics-driven approaches use knowledge about the case frames of the verbs to drive the interpretation process. Early semantic parsers often ignored syntax altogether [1, 2] although more recent systems tend to integrate the two components whether primarily syntax or semantics driven (e.g., [3]). Frame or task based parsers use infor-marion in the underlying domain to guide the parse. Script based parsers are one example of this class [4]. A more recent example was presented at last year's DARPA"
This chapter examines the application of natural language processing to computerassisted language learning including the history of work in this field over the last thirtyfive years but with a focus on current developments and opportunities.  36.1 
"In this report, some collaborative work between the fields of Machine Learning (ML) and Natural Language Processing (NLP) is presented. The document is structured in two parts. The first part includes a superficial but comprehensive survey covering the state--of--the--art of machine learning techniques applied to natural language learning tasks. In the second part, a particular problem, namely Word Sense Disambiguation (WSD), is studied in more detail. In doing so, four algorithms for supervised learning, which belong to different families, are compared in a benchmark corpus for the WSD task. Both qualitative and quantitative conclusions are drawn. This document stands for the complementary documentation for the conference ""Aprendizaje autom 'atico aplicado al procesamiento del lenguaje natural"", given by the author within the course: ""Curso de Industrias de la Lengua: La Ingenier'ia Lingu'istica en la Sociedad de la Informaci'on"", Fundaci'on Duques de Soria. Soria. July 2000. 1 Con..."
"ABSTRACT. In this special issue of TAL, we look at the fundamental principles underlying evaluation in natural language processing. We adopt a global point of view that goes beyond the horizon of a single evaluation campaign or a particular protocol. After a brief review of history and terminology, we will address the topic of a gold standard for natural language processing, of annotation quality, of the amount of data, of the difference between technology evaluation and usage evaluation, of dialog systems, and of standards, before concluding with a short discussion of the articles in this special issue and some prospective remarks. RÃ‰SUMÃ‰. Dans ce numÃ©ro spÃ©cial de TAL nous nous intÃ©ressons aux principes fondamentaux qui sous-tendent lâ€™Ã©valuation pour le traitement automatique du langage naturel, que nous abordons de maniÃ¨re globale, câ€™est Ã  dire au delÃ  de lâ€™horizon dâ€™une seule campagne dâ€™Ã©valuation ou dâ€™un protocole particulier. AprÃ¨s un rappel historique et terminologique, nous aborderons le sujet de la rÃ©fÃ©rence pour le traitement du langage naturel, de la qualitÃ© des annotations, de la quantitÃ© des donnÃ©es, des diffÃ©rence entre Ã©valuation de technologie et Ã©valuation dâ€™usage, de lâ€™Ã©valuation des systÃ¨mes de dialogue, des standards avant de conclure sur une bref prÃ©sentation des articles du numÃ©ro et quelques remarques prospectives."
"Abstract. This thesis examines the use of machine learning techniques in various tasks of natural language processing, mainly for the task of information extraction from texts. The objectives are the improvement of adaptability of information extraction systems to new thematic do-mains (or even languages), and the improvement of their performance using as fewer resources (either linguistic or human) as possible. This thesis has examined two main axes: a) the research and assessment of existing algorithms of machine learning mainly in the stages of linguistic pre-processing (such as part of speech tagging) and named-entity recog-nition, and b) the creation of a new machine learning algorithm and its assessment on synthetic data, as well as in real world data for the task of relation extraction between named entities. This new algorithm belongs to the category of inductive grammar learning, and can infer context free grammars from only positive examples."
"  Probabilistic finite-state string transducers (FSTs) are extremely popular in natural language processing, due to powerful generic methods for applying, composing, and learning them. Unfortunately, FSTs are not a good fit for much of the current work on probabilistic modeling for machine translation, summarization, paraphrasing, and language modeling. These methods operate directly on trees, rather than strings. We show that tree acceptors and tree transducers subsume most of this work, and we discuss algorithms for realizing the same benefits found in probabilistic string transduction. "
 
"Natural language processing systems (NLP) that extract clinical information from textual reports were shown to be effective for limited domains and for particular applications. Because an NLP system typically requires substantial resources to develop, it is beneficial if it is designed to be easily extendible to multiple domains and applications. This paper describes multiple extensions of an NLP system called MedLEE, which was originally developed for the domain of radiological reports of the chest, but has subsequently been extended to mammography, discharge summaries, all of radiology, electrocardiography, echocardiography, and pathology."
"We propose a bifurcated paradigm for the construction of a Prolog knowl-edge base from a body of documents: first, an information extraction (IE) ap-plication that will annotate the corpus and output the annotated documents, and second, a Prolog knowledge base (KB) application that will transform the annotated documents into a KB (a set of facts). The General Architecture for Text Engineering (GATE) was used as the platform for the development and execution of the IE application, which in-cluded most components of A Nearly New Information Extraction (ANNIE) system. Apart from the basic IE capabilities of ANNIE, the application fea-tured additional high-level annotation grammars written in the Java Annota-tion Patterns Engine (JAPE) language and a trainable annotator that used the maximum entropy machine learning model, which were designed to annotate several biographies of well-known mathematicians. The Prolog KB applica-tion, programmed to be executed within the XSB System, was designed to receive the annotated text output by the IE application and produce a knowl-edge base, and it successfully creates a database of Prolog facts that can be intelligently queried through the XSB System. The KB utilizes the frame representation of facts, specifically by treating one document as an object to be represented as a frame, with each annotation type treated as a slot whose multiple values are whichever specific strings were annotated by the IE appli-cation. This transformation of extracted information into Prolog facts forms a link between IE, a recent development in Natural Language Processing, and logic programming with Prolog. 1"
"We developed a prototype information retrieval system  which uses advanced natural language processing  techniques to enhance the effectiveness of traditional  key-word based document retrieval. The backbone  of our system is a statistical retrieval engine  which performs automated indexing of documents,  then search and ranking in response to user queries."
"In this paper we will discuss several issues and requirements for enabling natural language processing systems to become context-adaptive. Given the fact that emerging systems feature speaker independent continuous speech recognition restricted to individual domains and are equipped with syntactic and semantic parsers for understanding input from these domains, we should begin to envision open multi-domain natural language processing systems. We will address the following issues arising in such open systems: How to divide natural language processing modules in context-variant and context-invariant components and how to enable the ensuing systems to detect contextual changes for processing the context-variant phenomena.  "
"In Fall 2004 I introduced a new course  called Applied Natural Language Processing,  in which students acquire an understanding  of which text analysis techniques  are currently feasible for practical applications."
"This chapter examines the application of natural language processing to computerassisted language learning including the history of work in this field over the last thirtyfive years but with a focus on current developments and opportunities.  16.1 Introduction  This chapter focuses on applications of computational linguistics (CL) techniques to computer-assisted language learning (CALL). This always refers to programs designed to help people learn foreign languages, e.g., programs to help German high-school students learn French. CALL is a large field---much larger than computational linguistics---which no one can describe completely in a brief chapter. The focus of this chapter is therefore much narrower, viz., just on those areas of CALL to which CL has been applied or might be. CL programs process natural languages such as English and Spanish, and the techniques are therefore often referred to as natural language processing (NLP).  To preview this chapter's contents, we note that NL..."
"Previous work demonstrated that Web counts can be used to approximate bigram counts, suggesting that Web-based frequencies should be useful for a wide variety of Natural Language Processing (NLP) tasks. However, only a limited number of tasks have so far been tested using Web-scale data sets. The present article overcomes this limitation by systematically investigating the performance of Web-based models for several NLP tasks, covering both syntax and semantics, both generation and analysis, and a wider range of n-grams and parts of speech than have been previously explored. For the majority of our tasks, we find that simple, unsupervised models perform better when n-gram counts are obtained from the Web rather than from a large corpus. In some cases, performance can be improved further by using backoff or interpolation techniques that combine Web counts and corpus counts. However, unsupervised Web-based models generally fail to outperform supervised state-ofthe-art models trained on smaller corpora. We argue that Web-based models should therefore be used as a baseline for, rather than an alternative to, standard supervised models."
"This paper describes a natural language system which improves its own performance through learning. The system processes short English narratives and is able to acquire, from a single narrative, a new schema for a stereotypical set of actions. During the understanding process, the system attempts to construct explanations for characters ' actions in terms of the goals their actions were meant to achieve. When the system observes that a character has achieved an interesting goal in a novel way, it generalizes the set of actions they used to achieve this goal into a new schema. The generalization process is a knowledge-based analysis of the causal structure of the narrative which removes unnecessary details while maintaining the validity of the causal explanation. The resulting generalized set of actions is then stored as a new schema and used by the system to correctly process narratives which were previously beyond its capabilities. I"
 
"Natural Language Processing (NLP), which is a branch of artificial intelligence, includes speech synthesis, Speech recognition, and Machine translation. Natural Language Processing has a wide range of applications in the Indian context. Most of the rural Indian community is unable to make use of  the information technology revolution due to the dominance of English. Developments in the Natural processing Technology will offer universal access to information and services for more number of people in their mother language. The benefits that are expected to accrue as a result of widespread use of local language computing in India are discussed in this paper. India being a multilingual country with 22 official languages and about 1650 dialects, the research in Natural Language Processing  faces great challenges in India. This paper makes a survey  of the progress made in this field and explains the difficulties and challenges faced by the research community in the field of Natural Language Processing in India, and also suggests some practical solutions."
"An Evaluation of LOLITA and related Natural Language Processing Systems  Paul Callaghan  Submitted to the University of Durham for the degree of Ph.D., August 1997  ---------------------  This research addresses the question, ""how do we evaluate systems like LOLITA?"" LOLITA is the Natural Language Processing (NLP) system under development at the University of Durham. It is intended as a platform for building NL applications. We are therefore interested in questions of evaluation for such general NLP systems. The thesis has two parts."
"We classify and review current approaches  to software infrastructure for research, development  and delivery of NLP systems. The task"
"Gaussian Processes (GPs) are a powerful mod-elling framework incorporating kernels and Bayesian inference, and are recognised as state-of-the-art for many machine learning tasks."
"! lex-sign sense-id : sense-id dictionary ? = ""LDOCE"" ! lex-sign sense-id : sense-id ldb-entry-no ? = ""12364"" ! lex-sign sense-id : sense-id sense-no ? = ""0"".  When loaded into the LKB, (9) will be expanded into a fully-fledged representation for the transitive use of experience; by integrating word-specific information provided by (9) with the information encoded by the LKB type strict-trans-sign. Thus, although neither LDOCE, LLCE or the earlier subcategorised lexicon contain all the information about psychological verbs defined in Sanfilippo's type system, by using the conjunction of information available from all three, it proved possible to effectively enrich this information at the same time as mapping it into a formal representation.  4.2.5 Towards a Multilingual LKB  A goal of ACQUILEX is to demonstrate that an LKB can be produced that usefully exploits various MRD sources and integrates multilingual information. The use of a common LRL with a common type system, makes it possi..."
"Confidence measures are a practical solution for improving the usefulness of Natural Language Processing applications. Confidence estimation is a generic machine learning approach for deriving confidence measures. We give an overview of the application of confidence estimation in various fields of Natural Language Processing, and present experimental results for speech recognition, spoken language understanding, and statistical machine translation."
":  A fundamental issue in natural language processing is the prerequisite of an enormous quantity of preprogrammed knowledge concerning both the language and the domain under examination. Manual acquisition of this knowledge is tedious and error prone. Development of an automated acquisition process would prove invaluable. This paper references and overviews a range of the systems that have been developed in the domain of machine learning and natural language processing. Each system is categorised into either a symbolic or connectionist paradigm, and has its own characteristics and limitations described. Key words: machine learning, natural language processing, cognitive modelling, knowledge acquisition, knowledge representation.  page 2  1. INTRODUCTION 1.1 Artificial intelligence learning strategies  The study of machine learning in fields such as Artificial Intelligence (AI), Psychology and Neurology has been intense, but has attained limited success. In AI a wide range of strategi..."
"Kohonen's Self-Organizing Map (SOM) is one of the most popular artificial neural network algorithms. Word category maps are SOMs that have been organized according to word similarities, measured by the similarity of the short contexts of the words. Conceptually interrelated words tend to fall into the same or neighboring map nodes. Nodes may thus be viewed as word categories. Although no a priori information about classes is given, during the self-organizing process a model of the word classes emerges. The central topic of the thesis is the use of the SOM in natural language processing. The approach based on the word category maps is compared with the methods that are widely used in artificial intelligence research. Modeling gradience, conceptual change, and subjectivity of natural language interpretation are considered. The main application area is information retrieval and textual data mining for which a specific SOM-based method called the WEBSOM has been developed. The WEBSOM metho..."
"Abstract. Diff is a software program that detects differences between two data sets and is useful in natural language processing. This paper shows several examples of the application of diff. They include the detection of differences between two different datasets, extraction of rewriting rules, merging of two different datasets, and the optimal matching of two different data sets. Since diff comes with any standard UNIX system, it is readily available and very easy to use. Our studies showed that diff is a practical tool for research into natural language processing. 1"
"A general, reusable computational resource has been de- veloped within the Penman text generation project for organizing domain knowledge appropriately for linguistic realization. This resource, called the upper model, provides a domain- and task-independent classification system"" that supports sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguis- tic resources. This paper presents the results of our experiences in designing and using the upper model in a variety of applications over the past 5 years. In particular, we present our conclusions concerning the ap- propriate organization of an upper model, its domain- independence, and the types of interrelationships that need to be supported between upper model and gram- mar and semantics."
"Text statistics are frequently used in stylometry and cryptography studies. In this paper, some text statistics tools are developed in ISO Prolog for natural language processing. Details are given on the usage of 21 user-callable predicates. Logic and limitations of the program are also discussed. 1"
"This paper presents a workbench built by Priberam InformÃ¡tica for the development of the companyâ€™s natural language processing technology. This workbench includes a set of linguistic resources and software tools that have been applied in a considerable number of practical purposes, covering proofing tools, text processing and information retrieval. 1"
"Introduction  Natural language processing appears on the surface to be a strongly symbolic activity. Words are symbols that stand for objects and concepts in the real world, and they are put together into sentences that obey well-specified grammar rules. It is no surprise that for several decades natural language processing research has been dominated by the symbolic approach. Linguists have focused on describing language systems based on versions of the Universal Grammar. Artificial Intelligence researchers have built large programs where linguistic and world knowledge is expressed in symbolic structures, usually in LISP. Relatively little attention has been paid to various cognitive effects in language processing. Human language users perform differently from their linguistic competence, that is, from their knowledge of how to communicate correctly using language. Some linguistic structures (such as deep embeddings) are harder to deal with than others. People make mistakes wh"
"ABSTRACT: After twenty years of disfavor, a technology has returned which imitates the processes of the brain. Natural language experiments (Sejnowski & Rosenberg: 1986) demonstrate that neural network computing architecture can learn from actual spoken language, observe rules of pronunciation, and reproduce sounds from the patterns derived by its own processes. The consequences of neural network computing for natural language processing activities, including second language acquisition and representation, machine translation, and knowledge processing may be more convulsively revolutionary than anything imagined in current technology. This paper introduces neural network concepts to a traditional natural language processing audience."
"Natural language processing #NLP# programs are  confronted with various di#culties in processing HTML and XML documents, and have the potential to produce better results if linguistic information is annotated in the source texts. Wehave therefore developed the Linguistic Annotation Language  #or LAL#, which is an XML-compliant tag set for assisting natural language processing programs, and NLP tools such as parsers and machine translation programs which can accept LAL-annotated input. In addition, we have developed a LALannotation editor which allows users to annotate documents graphically without seeing tags. Further, we have conducted an experiment to check the translation quality improvementby using LAL annotation."
"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance. 1."
Research in Natural Language Processing (NLP) has in recent years benefited from the enormous amount of raw textual data available on the World Wide Web. The presence of standard search engines has made this data accessible to computational linguists as a corpus of a size that had never existed before. Although
"Abstract:- Natural language processing is process to handle the linguistic languages to understand the language in to more detailed format. It will providing us will the complete grammatical structure of the word of linguistic language into English. In the present system such as dictionary we only have the word with its meaning but it does provide us grammar. In our proposed system we are presenting the parsing tree which shows the complete grammatical structure.It will show all the tags chunks etc which help to understand the language in more detailed way. Keywords:- NLP, Morphological analyser, inflection rules, parse tree. I."
"This article was hard to follow, but it seenLed to simply illustrate that just about anything can be compiled into units and links given a sufficiently broad definition of those primitives"
  9. Bibliography  
